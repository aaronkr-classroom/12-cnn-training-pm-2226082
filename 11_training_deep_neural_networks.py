# -*- coding: utf-8 -*-
"""11_training_deep_neural_networks.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/rickiepark/handson-ml3/blob/main/11_training_deep_neural_networks.ipynb

**11ì¥ â€“ ì‹¬ì¸µ ì‹ ê²½ë§ í›ˆë ¨í•˜ê¸°**

_ì´ ë…¸íŠ¸ë¶ì—ëŠ” 11ì¥ì˜ ëª¨ë“  ìƒ˜í”Œ ì½”ë“œì™€ ì—°ìŠµ ë¬¸ì œì— ëŒ€í•œ ì†”ë£¨ì…˜ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤._

<table align="left">
  <td>
    <a href="https://colab.research.google.com/github/rickiepark/handson-ml3/blob/main/11_training_deep_neural_networks.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
  </td>
</table>

# ì„¤ì •

ì´ í”„ë¡œì íŠ¸ì—ëŠ” íŒŒì´ì¬ 3.7 ì´ìƒì´ í•„ìš”í•©ë‹ˆë‹¤:
"""

import sys

assert sys.version_info >= (3, 7)

"""ê·¸ë¦¬ê³  í…ì„œí”Œë¡œ â‰¥ 2.8:"""

from packaging import version
import tensorflow as tf

assert version.parse(tf.__version__) >= version.parse("2.8.0")

"""ì´ì „ ì¥ì—ì„œì™€ ë§ˆì°¬ê°€ì§€ë¡œ ê¸°ë³¸ ê¸€ê¼´ í¬ê¸°ë¥¼ ì •ì˜í•˜ì—¬ ê·¸ë¦¼ì„ ë” ì˜ˆì˜ê²Œ ë§Œë“¤ì–´ ë³´ê² ìŠµë‹ˆë‹¤:"""

import matplotlib.pyplot as plt

plt.rc('font', size=14)
plt.rc('axes', labelsize=14, titlesize=14)
plt.rc('legend', fontsize=14)
plt.rc('xtick', labelsize=10)
plt.rc('ytick', labelsize=10)

import sys
# ì½”ë©ì˜ ê²½ìš° ë‚˜ëˆ” í°íŠ¸ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.
if 'google.colab' in sys.modules:
    !sudo apt-get -qq -y install fonts-nanum
    import matplotlib.font_manager as fm
    font_files = fm.findSystemFonts(fontpaths=['/usr/share/fonts/truetype/nanum'])
    for fpath in font_files:
        fm.fontManager.addfont(fpath)

# ë‚˜ëˆ” í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
import matplotlib

matplotlib.rc('font', family='NanumBarunGothic')
matplotlib.rcParams['axes.unicode_minus'] = False

"""ê·¸ë¦¬ê³  `images/deep` í´ë”ë¥¼ ë§Œë“¤ê³ (ì•„ì§ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°), ì´ ë…¸íŠ¸ë¶ì„ í†µí•´ ì±…ì— ì‚¬ìš©í•  ê·¸ë¦¼ì„ ê³ í•´ìƒë„ë¡œ ì €ì¥í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” `save_fig()` í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ê² ìŠµë‹ˆë‹¤:"""

from pathlib import Path

IMAGES_PATH = Path() / "images" / "deep"
IMAGES_PATH.mkdir(parents=True, exist_ok=True)

def save_fig(fig_id, tight_layout=True, fig_extension="png", resolution=300):
    path = IMAGES_PATH / f"{fig_id}.{fig_extension}"
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format=fig_extension, dpi=resolution)

"""# ê·¸ë ˆì´ë””ì–¸íŠ¸ ì†Œì‹¤ê³¼ í­ì£¼ ë¬¸ì œ"""

# ì¶”ê°€ ì½”ë“œ - ì´ ì…€ì€ ê·¸ë¦¼ 11-1ì„ ìƒì„±í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤.

import numpy as np

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

z = np.linspace(-5, 5, 200)

plt.plot([-5, 5], [0, 0], 'k-')
plt.plot([-5, 5], [1, 1], 'k--')
plt.plot([0, 0], [-0.2, 1.2], 'k-')
plt.plot([-5, 5], [-3/4, 7/4], 'g--')
plt.plot(z, sigmoid(z), "b-", linewidth=2,
         label=r"$\sigma(z) = \dfrac{1}{1+e^{-z}}$")
props = dict(facecolor='black', shrink=0.1)
plt.annotate('í¬í™”', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props,
             fontsize=14, ha="center")
plt.annotate('í¬í™”', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props,
             fontsize=14, ha="center")
plt.annotate('ì„ í˜•', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props,
             fontsize=14, ha="center")
plt.grid(True)
plt.axis([-5, 5, -0.2, 1.2])
plt.xlabel("$z$")
plt.legend(loc="upper left", fontsize=16)

save_fig("sigmoid_saturation_plot")
plt.show()

"""## Xavier ì´ˆê¸°í™”ì™€ He ì´ˆê¸°í™”"""

dense = tf.keras.layers.Dense(50, activation="relu",
                              kernel_initializer="he_normal")

he_avg_init = tf.keras.initializers.VarianceScaling(scale=2., mode="fan_avg",
                                                    distribution="uniform")
dense = tf.keras.layers.Dense(50, activation="sigmoid",
                              kernel_initializer=he_avg_init)

"""## ìˆ˜ë ´í•˜ì§€ ì•ŠëŠ” í™œì„±í™” í•¨ìˆ˜

### LeakyReLU
"""

# ì¶”ê°€ ì½”ë“œ - ì´ ì…€ì€ ê·¸ë¦¼ 11-2ë¥¼ ìƒì„±í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤.

def leaky_relu(z, alpha):
    return np.maximum(alpha * z, z)

z = np.linspace(-5, 5, 200)
plt.plot(z, leaky_relu(z, 0.1), "b-", linewidth=2, label=r"$LeakyReLU(z) = max(\alpha z, z)$")
plt.plot([-5, 5], [0, 0], 'k-')
plt.plot([0, 0], [-1, 3.7], 'k-')
plt.grid(True)
props = dict(facecolor='black', shrink=0.1)
plt.annotate('í†µê³¼', xytext=(-3.5, 0.5), xy=(-5, -0.3), arrowprops=props,
             fontsize=14, ha="center")
plt.xlabel("$z$")
plt.axis([-5, 5, -1, 3.7])
plt.gca().set_aspect("equal")
plt.legend()

save_fig("leaky_relu_plot")
plt.show()

leaky_relu = tf.keras.layers.LeakyReLU(alpha=0.2)  # ê¸°ë³¸ê°’ alpha=0.3
dense = tf.keras.layers.Dense(50, activation=leaky_relu,
                              kernel_initializer="he_normal")

model = tf.keras.models.Sequential([
    # [...]  # ë‹¤ë¥¸ ì¸µ
    tf.keras.layers.Dense(50, kernel_initializer="he_normal"),  # í™œì„±í™” í•¨ìˆ˜ ì—†ìŒ
    tf.keras.layers.LeakyReLU(alpha=0.2),  # ë³„ë„ì˜ ì¸µìœ¼ë¡œ í™œì„±í™” í•¨ìˆ˜ ì¶”ê°€
    # [...]  # ë‹¤ë¥¸ ì¸µ
])

"""### ELU

í…ì„œí”Œë¡œì—ì„œ ELUë¥¼ êµ¬í˜„í•˜ëŠ” ê²ƒì€ ê°„ë‹¨í•©ë‹ˆë‹¤. ì¸µì„ ë§Œë“¤ ë•Œ í™œì„±í™” í•¨ìˆ˜ë¥¼ ì§€ì •í•˜ê³  He ì´ˆê¸°í™”ë¥¼ ì‚¬ìš©í•˜ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤:
"""

dense = tf.keras.layers.Dense(50, activation="elu",
                              kernel_initializer="he_normal")

"""### SELU

ê¸°ë³¸ì ìœ¼ë¡œ SELU í•˜ì´í¼íŒŒë¼ë¯¸í„°(`scale` ë° `alpha`)ëŠ” ê° ë‰´ëŸ°ì˜ í‰ê·  ì¶œë ¥ì´ 0, í‘œì¤€í¸ì°¨ê°€ 1ì— ê°€ê¹ê²Œ ìœ ì§€ë˜ë„ë¡ íŠœë‹ë©ë‹ˆë‹¤(ì±…ì—ì„œ ì„¤ëª…í•œ ëŒ€ë¡œ ì…ë ¥ë„ í‰ê·  0ê³¼ í‘œì¤€í¸ì°¨ 1ë¡œ í‘œì¤€í™”ë˜ê³  ë‹¤ë¥¸ ì œì•½ ì¡°ê±´ì´ ì¤€ìˆ˜ëœë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤). ì´ í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ 1,000ê°œ ì¸µì„ ê°€ì§„ ì‹¬ì¸µ ì‹ ê²½ë§ì—ì„œë„ ëª¨ë“  ì¸µì—ì„œ ëŒ€ëµ í‰ê·  0ê³¼ í‘œì¤€í¸ì°¨ 1ì„ ìœ ì§€í•˜ì—¬ ê¸°ìš¸ê¸°ê°€ í­ì£¼/ì†Œë©¸í•˜ëŠ” ë¬¸ì œë¥¼ í”¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
"""

# ì¶”ê°€ ì½”ë“œ - ì´ ì…€ì€ ê·¸ë¦¼ 11-3ì„ ìƒì„±í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤.

from scipy.special import erfc

# í‰ê·  0, í‘œì¤€ í¸ì°¨ 1ë¡œ ìì²´ ì •ê·œí™”í•˜ê¸° ìœ„í•œ alpha ë° sacle
# (ë…¼ë¬¸ì— ìˆëŠ” ì‹ 14 ì°¸ì¡°):
alpha_0_1 = -np.sqrt(2 / np.pi) / (erfc(1 / np.sqrt(2)) * np.exp(1 / 2) - 1)
scale_0_1 = (
    (1 - erfc(1 / np.sqrt(2)) * np.sqrt(np.e))
    * np.sqrt(2 * np.pi)
    * (
        2 * erfc(np.sqrt(2)) * np.e ** 2
        + np.pi * erfc(1 / np.sqrt(2)) ** 2 * np.e
        - 2 * (2 + np.pi) * erfc(1 / np.sqrt(2)) * np.sqrt(np.e)
        + np.pi
        + 2
    ) ** (-1 / 2)
)

def elu(z, alpha=1):
    return np.where(z < 0, alpha * (np.exp(z) - 1), z)

def selu(z, scale=scale_0_1, alpha=alpha_0_1):
    return scale * elu(z, alpha)

z = np.linspace(-5, 5, 200)
plt.plot(z, elu(z), "b-", linewidth=2, label=r"$z < 0$ ì´ë©´ ELU$_\alpha(z) = \alpha (e^z - 1)$, ì•„ë‹ˆë©´ $z$")
plt.plot(z, selu(z), "r--", linewidth=2, label=r"SELU$(z) = 1.05 \, $ELU$_{1.67}(z)$")
plt.plot([-5, 5], [0, 0], 'k-')
plt.plot([-5, 5], [-1, -1], 'k:', linewidth=2)
plt.plot([-5, 5], [-1.758, -1.758], 'k:', linewidth=2)
plt.plot([0, 0], [-2.2, 3.2], 'k-')
plt.grid(True)
plt.axis([-5, 5, -2.2, 3.2])
plt.xlabel("$z$")
plt.gca().set_aspect("equal")
plt.legend()

save_fig("elu_selu_plot")
plt.show()

"""SELU ì‚¬ìš©ë²•ì€ ê°„ë‹¨í•©ë‹ˆë‹¤:"""

dense = tf.keras.layers.Dense(50, activation="selu",
                              kernel_initializer="lecun_normal")

"""**ì¶”ê°€ ìë£Œ - SELUë¥¼ ì‚¬ìš©í•œ ìê¸° ì •ê·œí™”ëœ ë„¤íŠ¸ì›Œí¬ì˜ ì˜ˆì‹œ**

SELU í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ 100ê°œì˜ ì€ë‹‰ì¸µì´ ìˆëŠ” íŒ¨ì…˜ MNISTìš© ì‹ ê²½ë§ì„ ë§Œë“¤ì–´ ë³´ê² ìŠµë‹ˆë‹¤:
"""

tf.random.set_seed(42)
model = tf.keras.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=[28, 28]))
for layer in range(100):
    model.add(tf.keras.layers.Dense(100, activation="selu",
                                    kernel_initializer="lecun_normal"))
model.add(tf.keras.layers.Dense(10, activation="softmax"))

model.compile(loss="sparse_categorical_crossentropy",
              optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),
              metrics=["accuracy"])

"""ì´ì œ í›ˆë ¨í•´ ë´…ì‹œë‹¤. ì…ë ¥ê°’ì„ í‰ê·  0, í‘œì¤€í¸ì°¨ 1ë¡œ ìŠ¤ì¼€ì¼ë§í•˜ëŠ” ê²ƒì„ ìŠì§€ ë§ˆì„¸ìš”:"""

fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()
(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist
X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]
X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]
X_train, X_valid, X_test = X_train / 255, X_valid / 255, X_test / 255

class_names = ["T-shirt/top", "Trouser", "Pullover", "Dress", "Coat",
               "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot"]

pixel_means = X_train.mean(axis=0, keepdims=True)
pixel_stds = X_train.std(axis=0, keepdims=True)
X_train_scaled = (X_train - pixel_means) / pixel_stds
X_valid_scaled = (X_valid - pixel_means) / pixel_stds
X_test_scaled = (X_test - pixel_means) / pixel_stds

history = model.fit(X_train_scaled, y_train, epochs=5,
                    validation_data=(X_valid_scaled, y_valid))

"""ì‹ ê²½ë§ì´ ë§¤ìš° ê¹Šì€ë°ë„ í•™ìŠµì— ì„±ê³µí–ˆìŠµë‹ˆë‹¤. ì´ì œ ReLU í™œì„±í™” í•¨ìˆ˜ë¥¼ ëŒ€ì‹  ì‚¬ìš©í•˜ë©´ ì–´ë–¤ ì¼ì´ ë°œìƒí•˜ëŠ”ì§€ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤:"""

tf.random.set_seed(42)

model = tf.keras.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=[28, 28]))
for layer in range(100):
    model.add(tf.keras.layers.Dense(100, activation="relu",
                                    kernel_initializer="he_normal"))
model.add(tf.keras.layers.Dense(10, activation="softmax"))

model.compile(loss="sparse_categorical_crossentropy",
              optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),
              metrics=["accuracy"])

history = model.fit(X_train_scaled, y_train, epochs=5,
                    validation_data=(X_valid_scaled, y_valid))

"""ì¢‹ì§€ ì•Šë„¤ìš”. ê·¸ë ˆì´ë””ì–¸íŠ¸ ì†Œë©¸/í­ì£¼ ë¬¸ì œë¡œ ì¸í•´ ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤.

### GELU, Swish, Mish
"""

# ì¶”ê°€ ì½”ë“œ - ì´ ì…€ì€ ê·¸ë¦¼ 11-4ë¥¼ ìƒì„±í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤.

def swish(z, beta=1):
    return z * sigmoid(beta * z)

def approx_gelu(z):
    return swish(z, beta=1.702)

def softplus(z):
    return np.log(1 + np.exp(z))

def mish(z):
    return z * np.tanh(softplus(z))

z = np.linspace(-4, 2, 200)

beta = 0.6
plt.plot(z, approx_gelu(z), "b-", linewidth=2,
         label=r"GELU$(z) = z\,\Phi(z)$")
plt.plot(z, swish(z), "r--", linewidth=2,
         label=r"Swish$(z) = z\,\sigma(z)$")
plt.plot(z, swish(z, beta), "r:", linewidth=2,
         label=fr"Swish$_{{\beta={beta}}}(z)=z\,\sigma({beta}\,z)$")
plt.plot(z, mish(z), "g:", linewidth=3,
         label=fr"Mish$(z) = z\,\tanh($softplus$(z))$")
plt.plot([-4, 2], [0, 0], 'k-')
plt.plot([0, 0], [-2.2, 3.2], 'k-')
plt.grid(True)
plt.axis([-4, 2, -1, 2])
plt.gca().set_aspect("equal")
plt.xlabel("$z$")
plt.legend(loc="upper left")

save_fig("gelu_swish_mish_plot")
plt.show()

"""# ë°°ì¹˜ ì •ê·œí™”"""

# ì¶”ê°€ ì½”ë“œ - ì´ë¦„ ì¹´ìš´í„°ë¥¼ ì§€ìš°ê³  ëœë¤ ì‹œë“œë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
tf.keras.backend.clear_session()
tf.random.set_seed(42)

model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=[28, 28]),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(300, activation="relu",
                          kernel_initializer="he_normal"),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(100, activation="relu",
                          kernel_initializer="he_normal"),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(10, activation="softmax")
])

model.summary()

[(var.name, var.trainable) for var in model.layers[1].variables]

# ì¶”ê°€ ì½”ë“œ - ëª¨ë¸ì´ ì‘ë™í•œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤! ğŸ˜Š
model.compile(loss="sparse_categorical_crossentropy", optimizer="sgd",
              metrics="accuracy")
model.fit(X_train, y_train, epochs=2, validation_data=(X_valid, y_valid))

"""ë•Œë•Œë¡œ í™œì„±í™” í•¨ìˆ˜ ì „ì— BNì„ ì ìš©í•˜ëŠ” ê²ƒì´ ë” íš¨ê³¼ì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤(ì´ ì£¼ì œì— ëŒ€í•œ ë…¼ìŸì´ ìˆìŠµë‹ˆë‹¤). ë˜í•œ, `BatchNormalization` ì•ì˜ ì¸µì—ëŠ” í¸í–¥ í•­ì´ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë”°ë¼ì„œ í¸í–¥ì´ ìˆìœ¼ë©´ íŒŒë¼ë¯¸í„°ê°€ ë‚­ë¹„ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¸µì„ ìƒì„±í•  ë•Œ `use_bias=False`ë¡œ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:"""

# ì¶”ê°€ ì½”ë“œ - ì´ë¦„ ì¹´ìš´í„°ë¥¼ ì§€ìš°ê³  ë¬´ì‘ìœ„ ì‹œë“œë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
tf.keras.backend.clear_session()
tf.random.set_seed(42)

model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=[28, 28]),
    tf.keras.layers.Dense(300, kernel_initializer="he_normal", use_bias=False),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Activation("relu"),
    tf.keras.layers.Dense(100, kernel_initializer="he_normal", use_bias=False),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Activation("relu"),
    tf.keras.layers.Dense(10, activation="softmax")
])

# ì¶”ê°€ ì½”ë“œ - ëª¨ë¸ì´ ì‘ë™í•œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤! ğŸ˜Š
model.compile(loss="sparse_categorical_crossentropy", optimizer="sgd",
              metrics="accuracy")
model.fit(X_train, y_train, epochs=2, validation_data=(X_valid, y_valid))

"""## ê·¸ë ˆì´ë””ì–¸íŠ¸ í´ë¦¬í•‘

ëª¨ë“  `tf.keras.optimizers`ëŠ” `clipnorm` ë˜ëŠ” `clipvalue` ë§¤ê°œë³€ìˆ˜ë¥¼ ë°›ìŠµë‹ˆë‹¤:
"""

optimizer = tf.keras.optimizers.SGD(clipvalue=1.0)
model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer)

optimizer = tf.keras.optimizers.SGD(clipnorm=1.0)
model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer)

"""## ì‚¬ì „ í›ˆë ¨ëœ ì¸µ ì¬ì‚¬ìš©

### ì¼€ë¼ìŠ¤ ëª¨ë¸ ì¬ì‚¬ìš©

íŒ¨ì…˜ MNIST í›ˆë ¨ ì§‘í•©ì„ ë‘ ê°œë¡œ ë‚˜ëˆ  ë³´ê² ìŠµë‹ˆë‹¤:
* `X_train_A`: T-shirts/topsì™€ pulloversë¥¼ ì œì™¸í•œ ëª¨ë“  í•­ëª©ì˜ ëª¨ë“  ì´ë¯¸ì§€ (í´ë˜ìŠ¤ 0ê³¼ 2).
* `X_train_B`: T-shirts/topsì™€ pullovers ì¤‘ì—ì„œ ì²˜ìŒ 200ê°œì˜ ì´ë¯¸ì§€ë¡œë§Œ êµ¬ì„±ëœ í›¨ì”¬ ì‘ì€ í›ˆë ¨ ì„¸íŠ¸.

ê²€ì¦ ì„¸íŠ¸ì™€ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë„ ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ ë¶„í• ë˜ì§€ë§Œ ì´ë¯¸ì§€ ìˆ˜ì—ëŠ” ì œí•œì´ ì—†ìŠµë‹ˆë‹¤.

ì§‘í•© A(8ê°œì˜ í´ë˜ìŠ¤ê°€ ìˆëŠ” ë¶„ë¥˜ ì‘ì—…)ì— ëŒ€í•´ ëª¨ë¸ì„ í›ˆë ¨í•˜ê³ , ì´ë¥¼ ë‹¤ì‹œ ì§‘í•© B(ì´ì§„ ë¶„ë¥˜)ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë° ì‚¬ìš©í•˜ë ¤ê³  í•©ë‹ˆë‹¤. ì§‘í•© Aì˜ í´ë˜ìŠ¤(trousers, dresses, coats, sandals, shirts, sneakers, bags, and ankle boots)ëŠ” ì§‘í•© Bì˜ í´ë˜ìŠ¤(T-shirts/tops, pullovers)ê³¼ ë‹¤ì†Œ ìœ ì‚¬í•˜ë¯€ë¡œ, ì‘ì—… Aì—ì„œ ì‘ì—… Bë¡œ ì•½ê°„ì˜ ì§€ì‹ì´ ì „ì´ë˜ê¸°ë¥¼ í¬ë§í•©ë‹ˆë‹¤.

ê·¸ëŸ¬ë‚˜ `Dense` ì¸µì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ê°™ì€ ìœ„ì¹˜ì—ì„œ ë°œìƒí•˜ëŠ” íŒ¨í„´ë§Œ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤(í•˜ì§€ë§Œ 14ì¥ì—ì„œ ì‚´í´ë³¼ í•©ì„±ê³± ì¸µì€ í•™ìŠµëœ íŒ¨í„´ì„ ì´ë¯¸ì§€ì˜ ì–´ëŠ ìœ„ì¹˜ì—ì„œë‚˜ ê°ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤).
"""

# ì¶”ê°€ ì½”ë“œ - íŒ¨ì…˜ MNISTë¥¼ ì‘ì—… Aì™€ Bë¡œ ë¶„í• í•œ ë‹¤ìŒ ëª¨ë¸ Aë¥¼ í•™ìŠµí•˜ê³  "my_model_A"ì— ì €ì¥í•©ë‹ˆë‹¤.

pos_class_id = class_names.index("Pullover")
neg_class_id = class_names.index("T-shirt/top")

def split_dataset(X, y):
    y_for_B = (y == pos_class_id) | (y == neg_class_id)
    y_A = y[~y_for_B]
    y_B = (y[y_for_B] == pos_class_id).astype(np.float32)
    old_class_ids = list(set(range(10)) - set([neg_class_id, pos_class_id]))
    for old_class_id, new_class_id in zip(old_class_ids, range(8)):
        y_A[y_A == old_class_id] = new_class_id  # Aì— ëŒ€í•œ í´ë˜ìŠ¤ ID ì¬ì •ì˜
    return ((X[~y_for_B], y_A), (X[y_for_B], y_B))

(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)
(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)
(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)
X_train_B = X_train_B[:200]
y_train_B = y_train_B[:200]

tf.random.set_seed(42)

model_A = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=[28, 28]),
    tf.keras.layers.Dense(100, activation="relu",
                          kernel_initializer="he_normal"),
    tf.keras.layers.Dense(100, activation="relu",
                          kernel_initializer="he_normal"),
    tf.keras.layers.Dense(100, activation="relu",
                          kernel_initializer="he_normal"),
    tf.keras.layers.Dense(8, activation="softmax")
])

model_A.compile(loss="sparse_categorical_crossentropy",
                optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),
                metrics=["accuracy"])
history = model_A.fit(X_train_A, y_train_A, epochs=20,
                      validation_data=(X_valid_A, y_valid_A))
model_A.save("my_model_A")

# extra code â€“ train and evaluate model B, without reusing model A

tf.random.set_seed(42)
model_B = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=[28, 28]),
    tf.keras.layers.Dense(100, activation="relu",
                          kernel_initializer="he_normal"),
    tf.keras.layers.Dense(100, activation="relu",
                          kernel_initializer="he_normal"),
    tf.keras.layers.Dense(100, activation="relu",
                          kernel_initializer="he_normal"),
    tf.keras.layers.Dense(1, activation="sigmoid")
])

model_B.compile(loss="binary_crossentropy",
                optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),
                metrics=["accuracy"])
history = model_B.fit(X_train_B, y_train_B, epochs=20,
                      validation_data=(X_valid_B, y_valid_B))
model_B.evaluate(X_test_B, y_test_B)

"""ëª¨ë¸ BëŠ” í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ 91.85%ì˜ ì •í™•ë„ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. ì´ì œ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ Aë¥¼ ì¬ì‚¬ìš©í•´ ë³´ê² ìŠµë‹ˆë‹¤."""

model_A = tf.keras.models.load_model("my_model_A")
model_B_on_A = tf.keras.Sequential(model_A.layers[:-1])
model_B_on_A.add(tf.keras.layers.Dense(1, activation="sigmoid"))

"""`model_B_on_A`ì™€ `model_A`ëŠ” ì¸µì„ ì‹¤ì œ ê³µìœ í•˜ë¯€ë¡œ í•˜ë‚˜ë¥¼ í›ˆë ¨í•˜ë©´ ë‘ ëª¨ë¸ì´ ëª¨ë‘ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤. ì´ë¥¼ ë°©ì§€í•˜ë ¤ë©´ `model_A`ì˜ *ë³µì œí•˜ì—¬* `model_B_on_A`ë¥¼ ë§Œë“¤ì–´ì•¼ í•©ë‹ˆë‹¤:"""

tf.random.set_seed(42)  # ì¶”ê°€ ì½”ë“œ - ì¬í˜„ì„± ë³´ì¥

model_A_clone = tf.keras.models.clone_model(model_A)
model_A_clone.set_weights(model_A.get_weights())

# ì¶”ê°€ ì½”ë“œ - ì´ì „ ì…€ì—ì„œì™€ ë§ˆì°¬ê°€ì§€ë¡œ model_B_on_Aë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
model_B_on_A = tf.keras.Sequential(model_A_clone.layers[:-1])
model_B_on_A.add(tf.keras.layers.Dense(1, activation="sigmoid"))

for layer in model_B_on_A.layers[:-1]:
    layer.trainable = False

optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)
model_B_on_A.compile(loss="binary_crossentropy", optimizer=optimizer,
                     metrics=["accuracy"])

history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,
                           validation_data=(X_valid_B, y_valid_B))

for layer in model_B_on_A.layers[:-1]:
    layer.trainable = True

optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)
model_B_on_A.compile(loss="binary_crossentropy", optimizer=optimizer,
                     metrics=["accuracy"])
history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,
                           validation_data=(X_valid_B, y_valid_B))

"""ê·¸ë ‡ë‹¤ë©´ ìµœì¢… íŒê²°ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?"""

model_B_on_A.evaluate(X_test_B, y_test_B)

"""í›Œë¥­í•©ë‹ˆë‹¤! ëª¨ë¸ì˜ ì •í™•ë„ê°€ 91.85%ì—ì„œ 93.85%ë¡œ 2% í¬ì¸íŠ¸ ìƒìŠ¹í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” ì˜¤ë¥˜ìœ¨ì´ ê±°ì˜ 25% ê°ì†Œí–ˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤:"""

1 - (100 - 93.85) / (100 - 91.85)

"""# ê³ ê¸‰ ì˜µí‹°ë§ˆì´ì €"""

# ì¶”ê°€ ì½”ë“œ - íŒ¨ì…˜ MNISTì—ì„œ ì˜µí‹°ë§ˆì´ì €ë¥¼ í…ŒìŠ¤íŠ¸í•˜ëŠ” ê°„ë‹¨í•œ í•¨ìˆ˜

def build_model(seed=42):
    tf.random.set_seed(seed)
    return tf.keras.Sequential([
        tf.keras.layers.Flatten(input_shape=[28, 28]),
        tf.keras.layers.Dense(100, activation="relu",
                              kernel_initializer="he_normal"),
        tf.keras.layers.Dense(100, activation="relu",
                              kernel_initializer="he_normal"),
        tf.keras.layers.Dense(100, activation="relu",
                              kernel_initializer="he_normal"),
        tf.keras.layers.Dense(10, activation="softmax")
    ])

def build_and_train_model(optimizer):
    model = build_model()
    model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer,
                  metrics=["accuracy"])
    return model.fit(X_train, y_train, epochs=10,
                     validation_data=(X_valid, y_valid))

optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)

history_sgd = build_and_train_model(optimizer)  # ì¶”ê°€ ì½”ë“œ

"""## ëª¨ë©˜í…€ ìµœì í™”"""

optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)

history_momentum = build_and_train_model(optimizer)  # ì¶”ê°€ ì½”ë“œ

"""## ë„¤ìŠ¤í…Œë¡œí”„ ê°€ì† ê²½ì‚¬"""

optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9,
                                    nesterov=True)

history_nesterov = build_and_train_model(optimizer)  # ì¶”ê°€ ì½”ë“œ

"""## AdaGrad"""

optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.001)

history_adagrad = build_and_train_model(optimizer)  # ì¶”ê°€ ì½”ë“œ

"""## RMSProp"""

optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)

history_rmsprop = build_and_train_model(optimizer)  # ì¶”ê°€ ì½”ë“œ

"""## Adam"""

optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9,
                                     beta_2=0.999)

history_adam = build_and_train_model(optimizer)  # ì¶”ê°€ ì½”ë“œ

"""## Adamax"""

optimizer = tf.keras.optimizers.Adamax(learning_rate=0.001, beta_1=0.9,
                                       beta_2=0.999)

history_adamax = build_and_train_model(optimizer)  # ì¶”ê°€ ì½”ë“œ

"""## Nadam"""

optimizer = tf.keras.optimizers.Nadam(learning_rate=0.001, beta_1=0.9,
                                      beta_2=0.999)

history_nadam = build_and_train_model(optimizer)  # ì¶”ê°€ ì½”ë“œ

"""## AdamW

TF 1.12ë¶€í„° `AdamW`ê°€ experimentalì—ì„œ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤.
"""

optimizer = tf.keras.optimizers.AdamW(weight_decay=1e-5, learning_rate=0.001,
                                      beta_1=0.9, beta_2=0.999)

history_adamw = build_and_train_model(optimizer)  # ì¶”ê°€ ì½”ë“œ

# ì¶”ê°€ ì½”ë“œ - ëª¨ë“  ì˜µí‹°ë§ˆì´ì €ì˜ í•™ìŠµ ê³¡ì„  ì‹œê°í™”

for loss in ("loss", "val_loss"):
    plt.figure(figsize=(12, 8))
    opt_names = "SGD Momentum Nesterov AdaGrad RMSProp Adam Adamax Nadam AdamW"
    for history, opt_name in zip((history_sgd, history_momentum, history_nesterov,
                                  history_adagrad, history_rmsprop, history_adam,
                                  history_adamax, history_nadam, history_adamw),
                                 opt_names.split()):
        plt.plot(history.history[loss], label=f"{opt_name}", linewidth=3)

    plt.grid()
    plt.xlabel("ì—í¬í¬")
    plt.ylabel({"loss": "í›ˆë ¨ ì†ì‹¤", "val_loss": "ê²€ì¦ ì†ì‹¤"}[loss])
    plt.legend(loc="upper left")
    plt.axis([0, 9, 0.1, 0.7])
    plt.show()

"""## í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§

### ê±°ë“­ì œê³± ê¸°ë°˜ ìŠ¤ì¼€ì¤„ë§

learning_rate = initial_learning_rate / (1 + step / decay_steps)**power

ì¼€ë¼ìŠ¤ëŠ” `power = 1`ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

**ë…¸íŠ¸**: ì˜µí‹°ë§ˆì´ì €ì˜ `decay` ë§¤ê°œë³€ìˆ˜ëŠ” deprecated ë˜ì—ˆìŠµë‹ˆë‹¤. `decay` ë§¤ê°œë³€ìˆ˜ë¥¼ ê°€ì§„ êµ¬í˜• ì˜µí‹°ë§ˆì´ì €ëŠ” `tf.keras.optimizers.legacy`ì— ì•„ì§ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ê·¸ëŒ€ì‹  `tf.keras.optimizers.schedules`ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
"""

# DEPRECATED:
optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=0.01, decay=1e-4)

# RECOMMENDED:
lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(
    initial_learning_rate=0.01,
    decay_steps=10_000,
    decay_rate=1.0,
    staircase=False
)
optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)

"""`InverseTimeDecay` ìŠ¤ì¼€ì¤„ëŸ¬ëŠ” `learning_rate = initial_learning_rate / (1 + decay_rate * step / decay_step)`ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. `staircase = True`ë¡œ ì§€ì •í•˜ë©´ `step / decay_steps`ë¥¼ `floor(step / decay_step)`ìœ¼ë¡œ ë°”ê¿‰ë‹ˆë‹¤."""

history_power_scheduling = build_and_train_model(optimizer)  # ì¶”ê°€ ì½”ë“œ

# ì¶”ê°€ ì½”ë“œ - ì´ ì…€ì€ ê±°ë“­ì œê³± ê¸°ë°˜ ìŠ¤ì¼€ì¤„ë§ì„ í‘œì‹œí•©ë‹ˆë‹¤.

initial_learning_rate = 0.01
decay_rate = 1.0
decay_steps = 10_000

steps = np.arange(100_000)
lrs = initial_learning_rate / (1 + decay_rate * steps / decay_steps)
lrs2 = initial_learning_rate / (1 + decay_rate * np.floor(steps / decay_steps))

plt.plot(steps, lrs,  "-", label="staircase=False")
plt.plot(steps, lrs2,  "-", label="staircase=True")
plt.axis([0, steps.max(), 0, 0.0105])
plt.xlabel("ìŠ¤í…")
plt.ylabel("í•™ìŠµë¥ ")
plt.title("ê±°ë“­ì œê³± ê¸°ë°˜ ìŠ¤ì¼€ì¤„ë§", fontsize=14)
plt.legend()
plt.grid(True)
plt.show()

"""### ì§€ìˆ˜ ê¸°ë°˜ ìŠ¤ì¼€ì¤„ë§

```python
learning_rate = initial_learning_rate * decay_rate ** (step / decay_steps)
```
"""

lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate=0.01,
    decay_steps=20_000,
    decay_rate=0.1,
    staircase=False
)
optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)

history_exponential_scheduling = build_and_train_model(optimizer)  # ì¶”ê°€ ì½”ë“œ

# ì¶”ê°€ ì½”ë“œ - ì´ ì…€ì€ ì§€ìˆ˜ ê¸°ë°˜ ìŠ¤ì¼€ì¤„ë§ì„ í‘œì‹œí•©ë‹ˆë‹¤.

initial_learning_rate = 0.01
decay_rate = 0.1
decay_steps = 20_000

steps = np.arange(100_000)
lrs = initial_learning_rate * decay_rate ** (steps / decay_steps)
lrs2 = initial_learning_rate * decay_rate ** np.floor(steps / decay_steps)

plt.plot(steps, lrs,  "-", label="staircase=False")
plt.plot(steps, lrs2,  "-", label="staircase=True")
plt.axis([0, steps.max(), 0, 0.0105])
plt.xlabel("ìŠ¤í…")
plt.ylabel("í•™ìŠµë¥ ")
plt.title("ì§€ìˆ˜ ê¸°ë°˜ ìŠ¤ì¼€ì¤„ë§", fontsize=14)
plt.legend()
plt.grid(True)
plt.show()

"""ì¼€ë¼ìŠ¤ëŠ” ì‚¬ìš©ì ì •ì˜ ìŠ¤ì¼€ì¤„ë§ í•¨ìˆ˜ë¥¼ ìœ„í•´ `LearningRateScheduler` ì½œë°± í´ë˜ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ë¥¼ ì‚¬ìš©í•´ ì§€ìˆ˜ ê¸°ë°˜ ê°ì‡ ë¥¼ êµ¬í˜„í•˜ëŠ” ë°©ë²•ì„ ì‚´í´ ë³´ê²ŸìŠµë‹ˆë‹¤. ì—¬ê¸°ì—ì„œëŠ” ìŠ¤í…ì´ ì•„ë‹ˆë¼ ì—í¬í¬ë§ˆë‹¤ í•™ìŠµë¥ ì´ ë°”ë€ë‹ˆë‹¤."""

def exponential_decay_fn(epoch):
    return 0.01 * 0.1 ** (epoch / 20)

def exponential_decay(lr0, s):
    def exponential_decay_fn(epoch):
        return lr0 * 0.1 ** (epoch / s)
    return exponential_decay_fn

exponential_decay_fn = exponential_decay(lr0=0.01, s=20)

# ì¶”ê°€ ì½”ë“œ - íŒ¨ì…˜ MNISTìš© ëª¨ë¸ ë¹Œë“œ ë° ì»´íŒŒì¼

tf.random.set_seed(42)
model = build_model()
optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)
model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer,
              metrics=["accuracy"])

n_epochs = 20

lr_scheduler = tf.keras.callbacks.LearningRateScheduler(exponential_decay_fn)
history = model.fit(X_train, y_train, epochs=n_epochs,
                    validation_data=(X_valid, y_valid),
                    callbacks=[lr_scheduler])

"""ë˜ëŠ” ìŠ¤ì¼€ì¤„ í•¨ìˆ˜ê°€ í˜„ì¬ í•™ìŠµë¥ ì„ ë‘ ë²ˆì§¸ ì¸ìˆ˜ë¡œ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤:"""

def exponential_decay_fn(epoch, lr):
    return lr * 0.1 ** (1 / 20)

"""**ì¶”ê°€ ìë£Œ**: ê° ì—í¬í¬ê°€ ì•„ë‹Œ ê° ë°˜ë³µë§ˆë‹¤ í•™ìŠµë¥ ì„ ì—…ë°ì´íŠ¸í•˜ëŠ” ì‚¬ìš©ì ì •ì˜ ìŠ¤ì¼€ì¤„ë§ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ìì²´ ì½œë°± í´ë˜ìŠ¤ë¥¼ ì‘ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:"""

K = tf.keras.backend

class ExponentialDecay(tf.keras.callbacks.Callback):
    def __init__(self, n_steps=40_000):
        super().__init__()
        self.n_steps = n_steps

    def on_batch_begin(self, batch, logs=None):
        # ì°¸ê³ : `batch` ì¸ìˆ˜ëŠ” ê° ì—í¬í¬ë§ˆë‹¤ ì¬ì„¤ì •ë©ë‹ˆë‹¤.
        lr = K.get_value(self.model.optimizer.learning_rate)
        new_learning_rate = lr * 0.1 ** (1 / self.n_steps)
        K.set_value(self.model.optimizer.learning_rate, new_learning_rate)

    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        logs['lr'] = K.get_value(self.model.optimizer.learning_rate)

lr0 = 0.01
model = build_model()
optimizer = tf.keras.optimizers.SGD(learning_rate=lr0)
model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer,
              metrics=["accuracy"])

import math

batch_size = 32
n_steps = n_epochs * math.ceil(len(X_train) / batch_size)
exp_decay = ExponentialDecay(n_steps)
history = model.fit(X_train, y_train, epochs=n_epochs,
                    validation_data=(X_valid, y_valid),
                    callbacks=[exp_decay])

"""### êµ¬ê°„ë³„ ê³ ì • ìŠ¤ì¼€ì¤„ë§"""

lr_schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay(
    boundaries=[50_000, 80_000],
    values=[0.01, 0.005, 0.001]
)
optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)

history_piecewise_scheduling = build_and_train_model(optimizer)  # ì¶”ê°€ ì½”ë“œ

# ì¶”ê°€ ì½”ë“œ - ì´ ì…€ì€ êµ¬ê°„ë³„ ê³ ì • ìŠ¤ì¼€ì¤„ë§ì„ í‘œì‹œí•©ë‹ˆë‹¤.

boundaries = [50_000, 80_000]
values = [0.01, 0.005, 0.001]

steps = np.arange(100_000)

lrs = np.full(len(steps), values[0])
for boundary, value in zip(boundaries, values[1:]):
    lrs[boundary:] = value

plt.plot(steps, lrs, "-")
plt.axis([0, steps.max(), 0, 0.0105])
plt.xlabel("ìŠ¤í…")
plt.ylabel("í•™ìŠµë¥ ")
plt.title("êµ¬ê°„ë³„ ê³ ì • ìŠ¤ì¼€ì¤„ë§", fontsize=14)
plt.grid(True)
plt.show()

"""ì§€ìˆ˜ ê¸°ë°˜ ìŠ¤ì¼€ì¤„ë§ì—ì„œ í–ˆë˜ ê²ƒì²˜ëŸ¼ êµ¬ê°„ë³„ ê³ ì • ìŠ¤ì¼€ì¤„ë§ì„ ìˆ˜ë™ìœ¼ë¡œ êµ¬í˜„í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤:"""

def piecewise_constant_fn(epoch):
    if epoch < 5:
        return 0.01
    elif epoch < 15:
        return 0.005
    else:
        return 0.001

# ì¶”ê°€ ì½”ë“œ - ì´ ì…€ì€ êµ¬ê°„ë³„ ê³ ì • ìŠ¤ì¼€ì¤„ë§ì„ ì •ì˜í•˜ëŠ” ì¼ë°˜ì ì¸ ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

def piecewise_constant(boundaries, values):
    boundaries = np.array([0] + boundaries)
    values = np.array(values)
    def piecewise_constant_fn(epoch):
        return values[(boundaries > epoch).argmax() - 1]
    return piecewise_constant_fn

piecewise_constant_fn = piecewise_constant([5, 15], [0.01, 0.005, 0.001])

# ì¶”ê°€ ì½”ë“œ - ì´ì „ê³¼ ê°™ì´ tf.keras.callbacks.LearningRateSchedulerë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

n_epochs = 25

lr_scheduler = tf.keras.callbacks.LearningRateScheduler(piecewise_constant_fn)

model = build_model()
optimizer = tf.keras.optimizers.Nadam(learning_rate=lr0)
model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer,
              metrics=["accuracy"])
history = model.fit(X_train, y_train, epochs=n_epochs,
                    validation_data=(X_valid, y_valid),
                    callbacks=[lr_scheduler])

"""ì§€ê¸ˆê¹Œì§€ `InverseTimeDecay`, `ExponentialDecay`, `PiecewiseConstantDecay`ë¥¼ ì‚´í´ ë³´ì•˜ìŠµë‹ˆë‹¤. `tf.keras.optimizers.schedules`ì—ëŠ” ì´ì™¸ì— ëª‡ ê°œì˜ ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ë” ìˆìŠµë‹ˆë‹¤. ì „ì²´ ë¦¬ìŠ¤íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤."""

for name in sorted(dir(tf.keras.optimizers.schedules)):
    if name[0] == name[0].lower():  # must start with capital letter
        continue
    scheduler_class = getattr(tf.keras.optimizers.schedules, name)
    print(f"â€¢ {name} â€“ {scheduler_class.__doc__.splitlines()[0]}")

"""### ì„±ëŠ¥ ê¸°ë°˜ ìŠ¤ì¼€ì¤„ë§"""

# ì¶”ê°€ ì½”ë“œ - ëª¨ë¸ ë¹Œë“œ ë° ì»´íŒŒì¼

model = build_model()
optimizer = tf.keras.optimizers.SGD(learning_rate=lr0)
model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer,
              metrics=["accuracy"])

lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)
history = model.fit(X_train, y_train, epochs=n_epochs,
                    validation_data=(X_valid, y_valid),
                    callbacks=[lr_scheduler])

# ì¶”ê°€ ì½”ë“œ - ì´ ì…€ì€ ì„±ëŠ¥ ê¸°ë°˜ ìŠ¤ì¼€ì¤„ë§ì„ í‘œì‹œí•©ë‹ˆë‹¤.

plt.plot(history.epoch, history.history["lr"], "bo-")
plt.xlabel("ì—í¬í¬")
plt.ylabel("í•™ìŠµë¥ ", color='b')
plt.tick_params('y', colors='b')
plt.gca().set_xlim(0, n_epochs - 1)
plt.grid(True)

ax2 = plt.gca().twinx()
ax2.plot(history.epoch, history.history["val_loss"], "r^-")
ax2.set_ylabel('ê²€ì¦ ì†ì‹¤', color='r')
ax2.tick_params('y', colors='r')

plt.title("í‰íƒ„ ì§€ì—­ì—ì„œ LR ê°ì†Œí•˜ê¸°", fontsize=14)
plt.show()

"""### 1ì‚¬ì´í´ ìŠ¤ì¼€ì¤„ë§

`ExponentialLearningRate` ì‚¬ìš©ì ì§€ì • ì½œë°±ì€ í›ˆë ¨ì˜ ê° ë°°ì¹˜ê°€ ëë‚  ë•Œë§ˆë‹¤ í•™ìŠµë¥ ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. í•™ìŠµë¥ ì— ìƒìˆ˜ `factor`ë¥¼ ê³±í•©ë‹ˆë‹¤. ë˜í•œ ê° ë°°ì¹˜ì—ì„œ í•™ìŠµë¥ ê³¼ ì†ì‹¤ì„ ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. `logs["loss"]`ëŠ” ì‹¤ì œë¡œ ì—í¬í¬ ì‹œì‘ ì´í›„ì˜ í‰ê·  ì†ì‹¤ì¸ë° ìš°ë¦¬ëŠ” ëŒ€ì‹  ë°°ì¹˜ ì†ì‹¤ì„ ì €ì¥í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì—í¬í¬ ì‹œì‘ ì´í›„ ë°°ì¹˜ íšŸìˆ˜ì˜ í‰ê· ì„ ê³„ì‚°í•˜ì—¬ ì§€ê¸ˆê¹Œì§€ì˜ ì´ ì†ì‹¤ì„ êµ¬í•œ ë‹¤ìŒ ì´ì „ ë°°ì¹˜ì˜ ì´ ì†ì‹¤ì„ ë¹¼ì„œ í˜„ì¬ ë°°ì¹˜ì˜ ì†ì‹¤ì„ êµ¬í•´ì•¼ í•©ë‹ˆë‹¤.
"""

K = tf.keras.backend

class ExponentialLearningRate(tf.keras.callbacks.Callback):
    def __init__(self, factor):
        self.factor = factor
        self.rates = []
        self.losses = []

    def on_epoch_begin(self, epoch, logs=None):
        self.sum_of_epoch_losses = 0

    def on_batch_end(self, batch, logs=None):
        mean_epoch_loss = logs["loss"]  # ì§€ê¸ˆê¹Œì§€ì˜ í‰ê·  ì†ì‹¤
        new_sum_of_epoch_losses = mean_epoch_loss * (batch + 1)
        batch_loss = new_sum_of_epoch_losses - self.sum_of_epoch_losses
        self.sum_of_epoch_losses = new_sum_of_epoch_losses
        self.rates.append(K.get_value(self.model.optimizer.learning_rate))
        self.losses.append(batch_loss)
        K.set_value(self.model.optimizer.learning_rate,
                    self.model.optimizer.learning_rate * self.factor)

"""`find_learning_rate()` í•¨ìˆ˜ëŠ” `ExponentialLearningRate` ì½œë°±ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í›ˆë ¨í•˜ê³  í•™ìŠµë¥ ê³¼ í•´ë‹¹ ë°°ì¹˜ ì†ì‹¤ì„ ë°˜í™˜í•©ë‹ˆë‹¤. ë§ˆì§€ë§‰ì—ëŠ” ëª¨ë¸ê³¼ í•´ë‹¹ ì˜µí‹°ë§ˆì´ì €ë¥¼ ì´ˆê¸° ìƒíƒœë¡œ ë³µì›í•©ë‹ˆë‹¤."""

def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=1e-4,
                       max_rate=1):
    init_weights = model.get_weights()
    iterations = math.ceil(len(X) / batch_size) * epochs
    factor = (max_rate / min_rate) ** (1 / iterations)
    init_lr = K.get_value(model.optimizer.learning_rate)
    K.set_value(model.optimizer.learning_rate, min_rate)
    exp_lr = ExponentialLearningRate(factor)
    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,
                        callbacks=[exp_lr])
    K.set_value(model.optimizer.learning_rate, init_lr)
    model.set_weights(init_weights)
    return exp_lr.rates, exp_lr.losses

"""`plot_lr_vs_loss()` í•¨ìˆ˜ëŠ” í•™ìŠµë¥ ê³¼ ì†ì‹¤ì„ í”Œë¡¯í•©ë‹ˆë‹¤. 1ì‚¬ì´í´ì˜ ìµœëŒ€ í•™ìŠµë¥ ë¡œ ì‚¬ìš©í•  ìµœì ì˜ í•™ìŠµë¥ ì€ ê³¡ì„ ì˜ ë§¨ ì•„ë˜ì— ìˆìŠµë‹ˆë‹¤."""

def plot_lr_vs_loss(rates, losses):
    plt.plot(rates, losses, "b")
    plt.gca().set_xscale('log')
    max_loss = losses[0] + min(losses)
    plt.hlines(min(losses), min(rates), max(rates), color="k")
    plt.axis([min(rates), max(rates), 0, max_loss])
    plt.xlabel("í•™ìŠµë¥ ")
    plt.ylabel("ì†ì‹¤")
    plt.grid()

"""ê°„ë‹¨í•œ íŒ¨ì…˜ MNIST ëª¨ë¸ì„ ë¹Œë“œí•˜ê³  ì»´íŒŒì¼í•´ ë³´ê² ìŠµë‹ˆë‹¤:"""

model = build_model()
model.compile(loss="sparse_categorical_crossentropy",
              optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),
              metrics=["accuracy"])

"""ì´ì œ 1ì‚¬ì´í´ì— ëŒ€í•œ ìµœì ì˜ ìµœëŒ€ í•™ìŠµë¥ ì„ ì°¾ì•„ë³´ê² ìŠµë‹ˆë‹¤:"""

batch_size = 128
rates, losses = find_learning_rate(model, X_train, y_train, epochs=1,
                                   batch_size=batch_size)
plot_lr_vs_loss(rates, losses)

"""1ì‚¬ì´í´ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ í•™ìŠµë¥ ì€ 10<sup>-1</sup> ì •ë„ì¸ ê²ƒ ê°™ìŠµë‹ˆë‹¤.

`OneCycleScheduler` ì‚¬ìš©ì ì§€ì • ì½œë°±ì€ ê° ë°°ì¹˜ê°€ ì‹œì‘ë  ë•Œ í•™ìŠµë¥ ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. ì´ ì±…ì— ì„¤ëª…ëœ ë…¼ë¦¬ë¥¼ ì ìš©í•©ë‹ˆë‹¤. í›ˆë ¨ì˜ ì•½ ì ˆë°˜ ë™ì•ˆ í•™ìŠµ ì†ë„ë¥¼ ì„ í˜•ì ìœ¼ë¡œ ì¦ê°€ì‹œí‚¨ ë‹¤ìŒ, ë‹¤ì‹œ ì´ˆê¸° í•™ìŠµ ì†ë„ë¡œ ì„ í˜•ì ìœ¼ë¡œ ê°ì†Œì‹œí‚¤ê³ , ë§ˆì§€ë§‰ìœ¼ë¡œ í›ˆë ¨ì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ì—ì„œëŠ” í•™ìŠµ ì†ë„ë¥¼ ì„ í˜•ì ìœ¼ë¡œ 0ì— ê°€ê¹ê²Œ ê°ì†Œì‹œí‚µë‹ˆë‹¤.
"""

class OneCycleScheduler(tf.keras.callbacks.Callback):
    def __init__(self, iterations, max_lr=1e-3, start_lr=None,
                 last_iterations=None, last_lr=None):
        self.iterations = iterations
        self.max_lr = max_lr
        self.start_lr = start_lr or max_lr / 10
        self.last_iterations = last_iterations or iterations // 10 + 1
        self.half_iteration = (iterations - self.last_iterations) // 2
        self.last_lr = last_lr or self.start_lr / 1000
        self.iteration = 0

    def _interpolate(self, iter1, iter2, lr1, lr2):
        return (lr2 - lr1) * (self.iteration - iter1) / (iter2 - iter1) + lr1

    def on_batch_begin(self, batch, logs):
        if self.iteration < self.half_iteration:
            lr = self._interpolate(0, self.half_iteration, self.start_lr,
                                   self.max_lr)
        elif self.iteration < 2 * self.half_iteration:
            lr = self._interpolate(self.half_iteration, 2 * self.half_iteration,
                                   self.max_lr, self.start_lr)
        else:
            lr = self._interpolate(2 * self.half_iteration, self.iterations,
                                   self.start_lr, self.last_lr)
        self.iteration += 1
        K.set_value(self.model.optimizer.learning_rate, lr)

"""ê°„ë‹¨í•œ íŒ¨ì…˜ MNIST ëª¨ë¸ì„ ë¹Œë“œí•˜ê³  ì»´íŒŒì¼í•œ ë‹¤ìŒ `OneCycleScheduler` ì½œë°±ì„ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨í•´ ë³´ê² ìŠµë‹ˆë‹¤:"""

model = build_model()
model.compile(loss="sparse_categorical_crossentropy",
              optimizer=tf.keras.optimizers.SGD(),
              metrics=["accuracy"])
n_epochs = 25
onecycle = OneCycleScheduler(math.ceil(len(X_train) / batch_size) * n_epochs,
                             max_lr=0.1)
history = model.fit(X_train, y_train, epochs=n_epochs, batch_size=batch_size,
                    validation_data=(X_valid, y_valid),
                    callbacks=[onecycle])

"""# ê·œì œë¥¼ í†µí•œ ê³¼ì í•© ë°©ì§€

## $\ell_1$ê³¼ $\ell_2$ ê·œì œ
"""

layer = tf.keras.layers.Dense(100, activation="relu",
                              kernel_initializer="he_normal",
                              kernel_regularizer=tf.keras.regularizers.l2(0.01))

"""ë˜ëŠ” ê³„ìˆ˜ê°€ 0.1ì¸ â„“<sub>1</sub> ì •ê·œí™”ì˜ ê²½ìš° `l1(0.1)`ì„ ì‚¬ìš©í•˜ê±°ë‚˜, ê³„ìˆ˜ê°€ ê°ê° 0.1ê³¼ 0.01ì¸ â„“<sub>1</sub> ë° â„“<sub>2</sub> ì •ê·œí™”ì˜ ê²½ìš° `l1_l2(0.1, 0.01)`ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."""

tf.random.set_seed(42)  # ì¶”ê°€ ì½”ë“œ - ì¬í˜„ì„±ì„ ìœ„í•œ

from functools import partial

RegularizedDense = partial(tf.keras.layers.Dense,
                           activation="relu",
                           kernel_initializer="he_normal",
                           kernel_regularizer=tf.keras.regularizers.l2(0.01))

model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=[28, 28]),
    RegularizedDense(100),
    RegularizedDense(100),
    RegularizedDense(10, activation="softmax")
])

# ì¶”ê°€ ì½”ë“œ - ëª¨ë¸ ì»´íŒŒì¼ ë° í›ˆë ¨
optimizer = tf.keras.optimizers.SGD(learning_rate=0.02)
model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer,
              metrics=["accuracy"])
history = model.fit(X_train, y_train, epochs=2,
                    validation_data=(X_valid, y_valid))

"""## ë“œë¡­ì•„ì›ƒ"""

tf.random.set_seed(42)  # ì¶”ê°€ ì½”ë“œ - ì¬í˜„ì„±ì„ ìœ„í•œ

model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=[28, 28]),
    tf.keras.layers.Dropout(rate=0.2),
    tf.keras.layers.Dense(100, activation="relu",
                          kernel_initializer="he_normal"),
    tf.keras.layers.Dropout(rate=0.2),
    tf.keras.layers.Dense(100, activation="relu",
                          kernel_initializer="he_normal"),
    tf.keras.layers.Dropout(rate=0.2),
    tf.keras.layers.Dense(10, activation="softmax")
])

# ì¶”ê°€ ì½”ë“œ - ëª¨ë¸ ì»´íŒŒì¼ ë° í›ˆë ¨
optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)
model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer,
              metrics=["accuracy"])
history = model.fit(X_train, y_train, epochs=10,
                    validation_data=(X_valid, y_valid))

"""í›ˆë ¨ ì •í™•ë„ê°€ ê²€ì¦ ì •í™•ë„ë³´ë‹¤ ë‚®ì€ ê²ƒì²˜ëŸ¼ ë³´ì´ì§€ë§Œ ì´ëŠ” ë“œë¡­ì•„ì›ƒì´ í›ˆë ¨ ì¤‘ì—ë§Œ í™œì„±í™”ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. í›ˆë ¨ í›„(ì¦‰, ë“œë¡­ì•„ì›ƒì„ ëˆ ìƒíƒœì—ì„œ) í›ˆë ¨ ì„¸íŠ¸ì—ì„œ ëª¨ë¸ì„ í‰ê°€í•˜ë©´ ê²€ì¦ ì •í™•ë„ ë° í…ŒìŠ¤íŠ¸ ì •í™•ë„ë³´ë‹¤ ì•½ê°„ ë†’ì€ "ì‹¤ì œ" í›ˆë ¨ ì •í™•ë„ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤:"""

model.evaluate(X_train, y_train)

model.evaluate(X_test, y_test)

"""**ì°¸ê³ **: SELUë¥¼ ì‚¬ìš©í•˜ì—¬ ìê°€ ì •ê·œí™” ì‹ ê²½ë§ì„ êµ¬ì¶•í•˜ë ¤ë©´ `Dropout` ëŒ€ì‹  `AlphaDropout`ì„ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.

## MC ë“œë¡­ì•„ì›ƒ
"""

tf.random.set_seed(42)  # ì¶”ê°€ ì½”ë“œ - ì¬í˜„ì„±ì„ ìœ„í•œ

y_probas = np.stack([model(X_test, training=True)
                     for sample in range(100)])
y_proba = y_probas.mean(axis=0)

model.predict(X_test[:1]).round(3)

y_proba[0].round(3)

y_std = y_probas.std(axis=0)
y_std[0].round(3)

y_pred = y_proba.argmax(axis=1)
accuracy = (y_pred == y_test).sum() / len(y_test)
accuracy

class MCDropout(tf.keras.layers.Dropout):
    def call(self, inputs, training=None):
        return super().call(inputs, training=True)

# ì¶”ê°€ ì½”ë“œ - ì‹œí€€ì…œ ëª¨ë¸ì—ì„œ Dropoutì„ MCDropoutìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
Dropout = tf.keras.layers.Dropout
mc_model = tf.keras.Sequential([
    MCDropout(layer.rate) if isinstance(layer, Dropout) else layer
    for layer in model.layers
])
mc_model.set_weights(model.get_weights())

mc_model.summary()

"""ì´ì œ MC ë“œë¡­ì•„ì›ƒê³¼ í•¨ê»˜ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:"""

# ì¶”ê°€ ì½”ë“œ - ëª¨ë¸ì„ ì¬í›ˆë ¨í•˜ì§€ ì•Šê³  ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
tf.random.set_seed(42)
np.mean([mc_model.predict(X_test[:1])
         for sample in range(100)], axis=0).round(2)

"""## ë§¥ìŠ¤-ë…¸ë¦„"""

dense = tf.keras.layers.Dense(
    100, activation="relu", kernel_initializer="he_normal",
    kernel_constraint=tf.keras.constraints.max_norm(1.))

# ì¶”ê°€ ì½”ë“œ - ëª¨ë¸ì˜ ëª¨ë“  ì€ë‹‰ì¸µì— ë§¥ìŠ¤-ë…¸ë¦„ì„ ì ìš©í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
MaxNormDense = partial(tf.keras.layers.Dense,
                       activation="relu", kernel_initializer="he_normal",
                       kernel_constraint=tf.keras.constraints.max_norm(1.))

tf.random.set_seed(42)
model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=[28, 28]),
    MaxNormDense(100),
    MaxNormDense(100),
    tf.keras.layers.Dense(10, activation="softmax")
])
optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)
model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer,
              metrics=["accuracy"])
history = model.fit(X_train, y_train, epochs=10,
                    validation_data=(X_valid, y_valid))

"""# ì—°ìŠµë¬¸ì œ í•´ë‹µ

## 1. to 7.

ë¶€ë¡ A ì°¸ì¡°

## 8. CIFAR10ì— ë”¥ëŸ¬ë‹ ì ìš©í•˜ê¸°

### a.
*ë¬¸ì œ: 100ê°œì˜ ë‰´ëŸ°ì„ ê°€ì§„ ì€ë‹‰ì¸µ 20ê°œë¡œ ì‹¬ì¸µ ì‹ ê²½ë§ì„ ë§Œë“¤ì–´ë³´ì„¸ìš”(ë„ˆë¬´ ë§ì€ ê²ƒ ê°™ì§€ë§Œ ì´ ì—°ìŠµë¬¸ì œì˜ í•µì‹¬ì…ë‹ˆë‹¤). He ì´ˆê¸°í™”ì™€ Swish í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.*
"""

tf.random.set_seed(42)

model = tf.keras.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=[32, 32, 3]))
for _ in range(20):
    model.add(tf.keras.layers.Dense(100,
                                    activation="swish",
                                    kernel_initializer="he_normal"))

"""### b.
*ë¬¸ì œ: Nadam ì˜µí‹°ë§ˆì´ì €ì™€ ì¡°ê¸° ì¢…ë£Œë¥¼ ì‚¬ìš©í•˜ì—¬ CIFAR10 ë°ì´í„°ì…‹ì— ì´ ë„¤íŠ¸ì›Œí¬ë¥¼ í›ˆë ¨í•˜ì„¸ìš”. ã……`tf.keras.datasets.cifar10.load_ data()`ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ì ì¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ë°ì´í„°ì…‹ì€ 10ê°œì˜ í´ë˜ìŠ¤ì™€ 32Ã—32 í¬ê¸°ì˜ ì»¬ëŸ¬ ì´ë¯¸ì§€ 60,000ê°œë¡œ êµ¬ì„±ë©ë‹ˆë‹¤(50,000ê°œëŠ” í›ˆë ¨, 10,000ê°œëŠ” í…ŒìŠ¤íŠ¸). ë”°ë¼ì„œ 10ê°œì˜ ë‰´ëŸ°ê³¼ ì†Œí”„íŠ¸ë§¥ìŠ¤ í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ì¶œë ¥ì¸µì´ í•„ìš”í•©ë‹ˆë‹¤. ëª¨ë¸ êµ¬ì¡°ì™€ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ë°”ê¿€ ë•Œë§ˆë‹¤ ì ì ˆí•œ í•™ìŠµë¥ ì„ ì°¾ì•„ì•¼ í•œë‹¤ëŠ” ê²ƒì„ ê¸°ì–µí•˜ì„¸ìš”.*

ëª¨ë¸ì— ì¶œë ¥ì¸µì„ ì¶”ê°€í•©ë‹ˆë‹¤:
"""

model.add(tf.keras.layers.Dense(10, activation="softmax"))

"""í•™ìŠµë¥ ì´ 5e-5ì¸ Nadam ì˜µí‹°ë§ˆì´ì €ë¥¼ ì‚¬ìš©í•´ ë³´ê² ìŠµë‹ˆë‹¤. í•™ìŠµë¥  1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3 ë° 1e-2ë¥¼ ì‹œë„í•˜ê³  ê°ê° 10ê°œì˜ ì—í¬í¬ì— ëŒ€í•œ í•™ìŠµ ê³¡ì„ ì„ ë¹„êµí–ˆìŠµë‹ˆë‹¤(ì•„ë˜ TensorBoard ì½œë°±ì„ ì‚¬ìš©). 3e-5ì™€ 1e-4ì˜ í•™ìŠµë¥ ì´ ê½¤ ì¢‹ì•˜ê¸° ë•Œë¬¸ì— 5e-5ë¥¼ ì‚¬ìš©í•´ ë³´ì•˜ëŠ”ë°, ì´ í•™ìŠµë¥ ì´ ì•½ê°„ ë” ë‚˜ì€ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤."""

optimizer = tf.keras.optimizers.Nadam(learning_rate=5e-5)
model.compile(loss="sparse_categorical_crossentropy",
              optimizer=optimizer,
              metrics=["accuracy"])

"""CIFAR10 ë°ì´í„° ì„¸íŠ¸ë¥¼ ë¡œë“œí•´ ë³´ê² ìŠµë‹ˆë‹¤. ë˜í•œ ì¡°ê¸° ì¢…ë£Œë¥¼ ì‚¬ìš©í•˜ë ¤ê³  í•˜ë¯€ë¡œ ê²€ì¦ ì„¸íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì›ë³¸ í›ˆë ¨ ì„¸íŠ¸ì˜ ì²˜ìŒ 5,000ê°œì˜ ì´ë¯¸ì§€ë¥¼ ê²€ì¦ ì„¸íŠ¸ë¡œ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤:"""

cifar10 = tf.keras.datasets.cifar10.load_data()
(X_train_full, y_train_full), (X_test, y_test) = cifar10

X_train = X_train_full[5000:]
y_train = y_train_full[5000:]
X_valid = X_train_full[:5000]
y_valid = y_train_full[:5000]

"""ì´ì œ í•„ìš”í•œ ì½œë°±ì„ ìƒì„±í•˜ê³  ëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤:"""

early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=20,
                                                     restore_best_weights=True)
model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint("my_cifar10_model",
                                                         save_best_only=True)
run_index = 1 # ëª¨ë¸ì„ í›ˆë ¨í•  ë•Œë§ˆë‹¤ ì¦ê°€í•©ë‹ˆë‹¤.
run_logdir = Path() / "my_cifar10_logs" / f"run_{run_index:03d}"
tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)
callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir=./my_cifar10_logs

model.fit(X_train, y_train, epochs=100,
          validation_data=(X_valid, y_valid),
          callbacks=callbacks)

model.evaluate(X_valid, y_valid)

"""ê²€ì¦ ì†ì‹¤ì´ ê°€ì¥ ë‚®ì€ ëª¨ë¸ì€ ê²€ì¦ ì„¸íŠ¸ì—ì„œ ì•½ 46.8%ì˜ ì •í™•ë„ë¥¼ ì–»ì—ˆìŠµë‹ˆë‹¤. ê°€ì¥ ë‚®ì€ ê²€ì¦ ì†ì‹¤ì— ë„ë‹¬í•˜ëŠ” ë° 29ê°œì˜ ì—í¬í¬ê°€ ê±¸ë ¸ìœ¼ë©°, ì œ ë…¸íŠ¸ë¶(GPU ì—†ìŒ)ì—ì„œëŠ” ì—í¬í¬ë‹¹ ì•½ 10ì´ˆê°€ ê±¸ë ¸ìŠµë‹ˆë‹¤. ë°°ì¹˜ ì •ê·œí™”ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ê°œì„ í•  ìˆ˜ ìˆëŠ”ì§€ ì‚´í´ë´…ì‹œë‹¤.

### c.
*ë¬¸ì œ: ë°°ì¹˜ ì •ê·œí™”ë¥¼ ì¶”ê°€í•˜ê³  í•™ìŠµ ê³¡ì„ ì„ ë¹„êµí•´ë³´ì„¸ìš”. ì´ì „ë³´ë‹¤ ë¹ ë¥´ê²Œ ìˆ˜ë ´í•˜ë‚˜ìš”? ë” ì¢‹ì€ ëª¨ë¸ì´ ë§Œë“¤ì–´ì§€ë‚˜ìš”? í›ˆë ¨ ì†ë„ì—ëŠ” ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹˜ë‚˜ìš”?*

ì•„ë˜ ì½”ë“œëŠ” ìœ„ì˜ ì½”ë“œì™€ ë§¤ìš° ìœ ì‚¬í•˜ì§€ë§Œ ëª‡ ê°€ì§€ ë‹¤ë¥¸ ì ì´ ìˆìŠµë‹ˆë‹¤:

* ì¶œë ¥ì¸µì„ ì œì™¸í•œ ëª¨ë“  `Dense` ë ˆì´ì–´(í™œì„±í™” í•¨ìˆ˜ ì „) ë‹¤ìŒì— BN ë ˆì´ì–´ë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.
* í•™ìŠµë¥ ì„ 5e-4ë¡œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤. 1e-5, 3e-5, 5e-5, 1e-4, 3e-4, 5e-4, 1e-3, 3e-3ìœ¼ë¡œ ì‹¤í—˜í•œ ê²°ê³¼ 20íšŒ ì—í¬í¬ í›„ ê°€ì¥ ì¢‹ì€ ê²€ì¦ ì„±ëŠ¥ì„ ë³´ì¸ ê²ƒì„ ì„ íƒí–ˆìŠµë‹ˆë‹¤.
* ì‹¤í–‰ ë””ë ‰í„°ë¦¬ì˜ ì´ë¦„ì„ run_bn_*ë¡œ, ëª¨ë¸ íŒŒì¼ ì´ë¦„ì„ `my_cifar10_bn_model`ë¡œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤.
"""

tf.random.set_seed(42)

model = tf.keras.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=[32, 32, 3]))
for _ in range(20):
    model.add(tf.keras.layers.Dense(100, kernel_initializer="he_normal"))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.Activation("swish"))

model.add(tf.keras.layers.Dense(10, activation="softmax"))

optimizer = tf.keras.optimizers.Nadam(learning_rate=5e-4)
model.compile(loss="sparse_categorical_crossentropy",
              optimizer=optimizer,
              metrics=["accuracy"])

early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=20,
                                                     restore_best_weights=True)
model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint("my_cifar10_bn_model",
                                                         save_best_only=True)
run_index = 1 # ëª¨ë¸ì„ í›ˆë ¨í•  ë•Œë§ˆë‹¤ ì¦ê°€í•©ë‹ˆë‹¤.
run_logdir = Path() / "my_cifar10_logs" / f"run_bn_{run_index:03d}"
tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)
callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]

model.fit(X_train, y_train, epochs=100,
          validation_data=(X_valid, y_valid),
          callbacks=callbacks)

model.evaluate(X_valid, y_valid)

"""* *ëª¨ë¸ì´ ì´ì „ë³´ë‹¤ ë” ë¹ ë¥´ê²Œ ìˆ˜ë ´í•˜ê³  ìˆë‚˜ìš”?* í›¨ì”¬ ë¹¨ë¼ì¡ŒìŠµë‹ˆë‹¤! ì´ì „ ëª¨ë¸ì€ ìµœì € ê²€ì¦ ì†ì‹¤ì— ë„ë‹¬í•˜ëŠ” ë° 29ê°œì˜ ì—í¬í¬ê°€ ê±¸ë ¸ì§€ë§Œ, ìƒˆë¡œìš´ ëª¨ë¸ì€ ë‹¨ 12ê°œì˜ ì—í¬í¬ì—ì„œ ë™ì¼í•œ ì†ì‹¤ì„ ë‹¬ì„±í•˜ê³  17ë²ˆì§¸ ì—í¬í¬ê¹Œì§€ ê³„ì† ë°œì „í–ˆìŠµë‹ˆë‹¤. BN ë ˆì´ì–´ë¥¼ í†µí•´ í•™ìŠµì´ ì•ˆì •í™”ë˜ê³  í›¨ì”¬ ë” í° í•™ìŠµ ì†ë„ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆì—ˆê¸° ë•Œë¬¸ì— ìˆ˜ë ´ë„ ë” ë¹¨ë¼ì¡ŒìŠµë‹ˆë‹¤.
* *BNì´ ë” ë‚˜ì€ ëª¨ë¸ì„ ìƒì„±í•˜ë‚˜ìš”?* ë„¤! ìµœì¢… ëª¨ë¸ì˜ ê²€ì¦ ì •í™•ë„ë„ 46.7%ê°€ ì•„ë‹Œ 50.7%ë¡œ í›¨ì”¬ ë” ì¢‹ì•„ì¡ŒìŠµë‹ˆë‹¤. ì—¬ì „íˆ ì•„ì£¼ ì¢‹ì€ ëª¨ë¸ì€ ì•„ë‹ˆì§€ë§Œ ì ì–´ë„ ì´ì „ë³´ë‹¤ëŠ” í›¨ì”¬ ë‚˜ì•„ì¡ŒìŠµë‹ˆë‹¤(í•©ì„±ê³± ì‹ ê²½ë§ì´ë¼ë©´ í›¨ì”¬ ë” ì˜í•  ìˆ˜ ìˆì§€ë§Œ ì´ëŠ” ë‹¤ë¥¸ ì£¼ì œì…ë‹ˆë‹¤. 14ì¥ ì°¸ì¡°).
* *BNì´ í›ˆë ¨ ì†ë„ì— ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì³¤ë‚˜ìš”?* ëª¨ë¸ì´ í›¨ì”¬ ë¹ ë¥´ê²Œ ìˆ˜ë ´í–ˆì§€ë§Œ, BN ì¸µì— í•„ìš”í•œ ì¶”ê°€ ê³„ì‚°ìœ¼ë¡œ ì¸í•´ ê° ì—í¬í¬ë§ˆë‹¤ 10ì´ˆê°€ ì•„ë‹Œ ì•½ 15ì´ˆê°€ ê±¸ë ¸ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì „ì²´ì ìœ¼ë¡œ ìµœì ì˜ ëª¨ë¸ì— ë„ë‹¬í•˜ëŠ” ë° ê±¸ë¦¬ëŠ” í›ˆë ¨ ì‹œê°„(ì‹¤ì œ ì‹œê°„)ì€ ì•½ 10% ì •ë„ ë‹¨ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤.

### d.
*ë¬¸ì œ: ë°°ì¹˜ ì •ê·œí™”ë¥¼ SELUë¡œ ë°”ê¾¸ì–´ë³´ì„¸ìš”. ë„¤íŠ¸ì›Œí¬ê°€ ìê¸° ì •ê·œí™”í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ë³€ê²½ ì‚¬í•­ì„ ì ìš©í•´ë³´ì„¸ìš”(ì¦‰, ì…ë ¥ íŠ¹ì„± í‘œì¤€í™”, ë¥´ì¿¤ ì •ê·œë¶„í¬ ì´ˆê¸°í™”, ì™„ì „ ì—°ê²° ì¸µë§Œ ìˆœì°¨ì ìœ¼ë¡œ ìŒ“ì€ ì‹¬ì¸µ ì‹ ê²½ë§ ë“±).*
"""

tf.random.set_seed(42)

model = tf.keras.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=[32, 32, 3]))
for _ in range(20):
    model.add(tf.keras.layers.Dense(100,
                                    kernel_initializer="lecun_normal",
                                    activation="selu"))

model.add(tf.keras.layers.Dense(10, activation="softmax"))

optimizer = tf.keras.optimizers.Nadam(learning_rate=7e-4)
model.compile(loss="sparse_categorical_crossentropy",
              optimizer=optimizer,
              metrics=["accuracy"])

early_stopping_cb = tf.keras.callbacks.EarlyStopping(
    patience=20, restore_best_weights=True)
model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(
    "my_cifar10_selu_model", save_best_only=True)
run_index = 1 # ëª¨ë¸ì„ í›ˆë ¨í•  ë•Œë§ˆë‹¤ ì¦ê°€í•©ë‹ˆë‹¤.
run_logdir = Path() / "my_cifar10_logs" / f"run_selu_{run_index:03d}"
tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)
callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]

X_means = X_train.mean(axis=0)
X_stds = X_train.std(axis=0)
X_train_scaled = (X_train - X_means) / X_stds
X_valid_scaled = (X_valid - X_means) / X_stds
X_test_scaled = (X_test - X_means) / X_stds

model.fit(X_train_scaled, y_train, epochs=100,
          validation_data=(X_valid_scaled, y_valid),
          callbacks=callbacks)

model.evaluate(X_valid_scaled, y_valid)

"""ì´ ëª¨ë¸ì€ ë‹¨ 8ê°œì˜ ì—í¬í¬ ë§Œì— ì²« ë²ˆì§¸ ëª¨ë¸ì˜ ê²€ì¦ ì†ì‹¤ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. 14ê°œì˜ ì—í¬í¬ í›„ì— ì•½ 50.3%ì˜ ì •í™•ë„ë¡œ ê°€ì¥ ë‚®ì€ ê²€ì¦ ì†ì‹¤ì— ë„ë‹¬í–ˆëŠ”ë°, ì´ëŠ” ì›ë˜ ëª¨ë¸(46.7%)ë³´ë‹¤ëŠ” ì¢‹ì§€ë§Œ ë°°ì¹˜ ì •ê·œí™”ë¥¼ ì‚¬ìš©í•œ ëª¨ë¸(50.7%)ì—ëŠ” ë¯¸ì¹˜ì§€ ëª»í•˜ëŠ” ìˆ˜ì¹˜ì…ë‹ˆë‹¤. ê° ì—í¬í¬ëŠ” 9ì´ˆë°–ì— ê±¸ë¦¬ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì§€ê¸ˆê¹Œì§€ í•™ìŠµí•˜ëŠ” ë° ê°€ì¥ ë¹ ë¥¸ ëª¨ë¸ì…ë‹ˆë‹¤.

### e.
*ë¬¸ì œ: ì•ŒíŒŒ ë“œë¡­ì•„ì›ƒìœ¼ë¡œ ëª¨ë¸ì— ê·œì œë¥¼ ì ìš©í•´ë³´ì„¸ìš”. ê·¸ë‹¤ìŒ ëª¨ë¸ì„ ë‹¤ì‹œ í›ˆë ¨í•˜ì§€ ì•Šê³  MC ë“œë¡­ì•„ì›ƒìœ¼ë¡œ ë” ë†’ì€ ì •í™•ë„ë¥¼ ì–»ì„ ìˆ˜ ìˆëŠ”ì§€ í™•ì¸í•´ë³´ì„¸ìš”.*
"""

tf.random.set_seed(42)

model = tf.keras.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=[32, 32, 3]))
for _ in range(20):
    model.add(tf.keras.layers.Dense(100,
                                    kernel_initializer="lecun_normal",
                                    activation="selu"))

model.add(tf.keras.layers.AlphaDropout(rate=0.1))
model.add(tf.keras.layers.Dense(10, activation="softmax"))

optimizer = tf.keras.optimizers.Nadam(learning_rate=5e-4)
model.compile(loss="sparse_categorical_crossentropy",
              optimizer=optimizer,
              metrics=["accuracy"])

early_stopping_cb = tf.keras.callbacks.EarlyStopping(
    patience=20, restore_best_weights=True)
model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(
    "my_cifar10_alpha_dropout_model", save_best_only=True)
run_index = 1 # ëª¨ë¸ì„ í›ˆë ¨í•  ë•Œë§ˆë‹¤ ì¦ê°€í•©ë‹ˆë‹¤.
run_logdir = Path() / "my_cifar10_logs" / f"run_alpha_dropout_{run_index:03d}"
tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)
callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]

X_means = X_train.mean(axis=0)
X_stds = X_train.std(axis=0)
X_train_scaled = (X_train - X_means) / X_stds
X_valid_scaled = (X_valid - X_means) / X_stds
X_test_scaled = (X_test - X_means) / X_stds

model.fit(X_train_scaled, y_train, epochs=100,
          validation_data=(X_valid_scaled, y_valid),
          callbacks=callbacks)

model.evaluate(X_valid_scaled, y_valid)

"""ì´ ëª¨ë¸ì€ ê²€ì¦ ì„¸íŠ¸ì—ì„œ 48.1%ì˜ ì •í™•ë„ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” ë“œë¡­ì•„ì›ƒì´ ì—†ëŠ” ê²½ìš°(50.3%)ë³´ë‹¤ ë‚˜ì©ë‹ˆë‹¤. ê´‘ë²”ìœ„í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ì„ ì‚¬ìš©í•˜ë©´ ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆì§€ë§Œ(ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨ 5%, 10%, 20%, 40%, í•™ìŠµë¥  1e-4, 3e-4, 5e-4, 1e-3ìœ¼ë¡œ ì‹œë„í•´ ë³´ì•˜ìŠµë‹ˆë‹¤), ì´ ê²½ìš°ì—ëŠ” í¬ê²Œ ë‚˜ì•„ì§€ì§€ ì•Šì„ ê²ƒì…ë‹ˆë‹¤.

ì´ì œ MC ë“œë¡­ì•„ì›ƒì„ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤. ì•ì„œ ì‚¬ìš©í•œ `MCAlphaDropout` í´ë˜ìŠ¤ê°€ í•„ìš”í•˜ë¯€ë¡œ í¸ì˜ë¥¼ ìœ„í•´ ì—¬ê¸°ì— ë³µì‚¬í•´ ë‘ê² ìŠµë‹ˆë‹¤:
"""

class MCAlphaDropout(tf.keras.layers.AlphaDropout):
    def call(self, inputs):
        return super().call(inputs, training=True)

"""ì´ì œ ë°©ê¸ˆ í›ˆë ¨í•œ ëª¨ë¸ê³¼ ë™ì¼í•œ ëª¨ë¸(ë™ì¼í•œ ê°€ì¤‘ì¹˜)ì„ ë§Œë“¤ë˜, `AlphaDropout` ì¸µ ëŒ€ì‹  `MCAlphaDropout` ë“œë¡­ì•„ì›ƒ ì¸µì„ ì‚¬ìš©í•˜ëŠ” ìƒˆ ëª¨ë¸ì„ ë§Œë“¤ì–´ ë³´ê² ìŠµë‹ˆë‹¤:"""

mc_model = tf.keras.Sequential([
    (
        MCAlphaDropout(layer.rate)
        if isinstance(layer, tf.keras.layers.AlphaDropout)
        else layer
    )
    for layer in model.layers
])

"""ê·¸ëŸ° ë‹¤ìŒ ëª‡ ê°€ì§€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë¥¼ ì¶”ê°€í•´ ë³´ê² ìŠµë‹ˆë‹¤. ì²« ë²ˆì§¸ í•¨ìˆ˜ëŠ” ëª¨ë¸ì„ ì—¬ëŸ¬ ë²ˆ(ê¸°ë³¸ê°’ì€ 10íšŒ) ì‹¤í–‰í•˜ì—¬ ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ í‰ê·  í™•ë¥ ì„ ë°˜í™˜í•©ë‹ˆë‹¤. ë‘ ë²ˆì§¸ í•¨ìˆ˜ëŠ” ì´ í‰ê·  í™•ë¥ ì„ ì‚¬ìš©í•˜ì—¬ ê° ìƒ˜í”Œì— ëŒ€í•´ ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤:"""

def mc_dropout_predict_probas(mc_model, X, n_samples=10):
    Y_probas = [mc_model.predict(X) for sample in range(n_samples)]
    return np.mean(Y_probas, axis=0)

def mc_dropout_predict_classes(mc_model, X, n_samples=10):
    Y_probas = mc_dropout_predict_probas(mc_model, X, n_samples)
    return Y_probas.argmax(axis=1)

"""ì´ì œ ê²€ì¦ ì„¸íŠ¸ì˜ ëª¨ë“  ìƒ˜í”Œì— ëŒ€í•´ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ê³  ì •í™•ë„ë¥¼ ê³„ì‚°í•´ ë³´ê² ìŠµë‹ˆë‹¤:"""

tf.random.set_seed(42)

y_pred = mc_dropout_predict_classes(mc_model, X_valid_scaled)
accuracy = (y_pred == y_valid[:, 0]).mean()
accuracy

"""ì´ ê²½ìš° ë“œë¡­ì•„ì›ƒì´ ì—†ëŠ” ëª¨ë¸ì˜ ëŒ€ëµì ì¸ ì •í™•ë„(ì•½ 50.3% ì •í™•ë„)ì™€ ë¹„ìŠ·í•©ë‹ˆë‹¤.

ë”°ë¼ì„œ ì´ ì—°ìŠµë¬¸ì œì—ì„œ ê°€ì¥ ì¢‹ì€ ëª¨ë¸ì€ ë°°ì¹˜ ì •ê·œí™” ëª¨ë¸ì…ë‹ˆë‹¤.

### f.
*ë¬¸ì œ: 1ì‚¬ì´í´ ìŠ¤ì¼€ì¤„ë§ìœ¼ë¡œ ëª¨ë¸ì„ ë‹¤ì‹œ í›ˆë ¨í•˜ê³  í›ˆë ¨ ì†ë„ì™€ ëª¨ë¸ ì •í™•ë„ê°€ í–¥ìƒë˜ëŠ”ì§€ í™•ì¸í•´ë³´ì„¸ìš”.*
"""

tf.random.set_seed(42)

model = tf.keras.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=[32, 32, 3]))
for _ in range(20):
    model.add(tf.keras.layers.Dense(100,
                                    kernel_initializer="lecun_normal",
                                    activation="selu"))

model.add(tf.keras.layers.AlphaDropout(rate=0.1))
model.add(tf.keras.layers.Dense(10, activation="softmax"))

optimizer = tf.keras.optimizers.SGD()
model.compile(loss="sparse_categorical_crossentropy",
              optimizer=optimizer,
              metrics=["accuracy"])

# ì§€ìˆ˜ ë¶€ë¶„ì˜ ìŒìˆ˜ í‘œì‹œë¥¼ ìœ„í•´
# https://jehyunlee.github.io/2020/02/13/Python-DS-2-matplotlib_defaults_and_fonts/
plt.xticks(fontname="Liberation Sans")

batch_size = 128
rates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs=1,
                                   batch_size=batch_size)
plot_lr_vs_loss(rates, losses)

tf.random.set_seed(42)

model = tf.keras.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=[32, 32, 3]))
for _ in range(20):
    model.add(tf.keras.layers.Dense(100,
                                 kernel_initializer="lecun_normal",
                                 activation="selu"))

model.add(tf.keras.layers.AlphaDropout(rate=0.1))
model.add(tf.keras.layers.Dense(10, activation="softmax"))

optimizer = tf.keras.optimizers.SGD(learning_rate=2e-2)
model.compile(loss="sparse_categorical_crossentropy",
              optimizer=optimizer,
              metrics=["accuracy"])

n_epochs = 15
n_iterations = math.ceil(len(X_train_scaled) / batch_size) * n_epochs
onecycle = OneCycleScheduler(n_iterations, max_lr=0.05)
history = model.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size,
                    validation_data=(X_valid_scaled, y_valid),
                    callbacks=[onecycle])

"""1ì‚¬ì´í´ ë°©ì‹ìœ¼ë¡œ ë‹¨ 15 ì—í¬í¬ ë™ì•ˆì— ëª¨ë¸ì„ í›ˆë ¨í•  ìˆ˜ ìˆì—ˆìœ¼ë©°, ë°°ì¹˜ í¬ê¸°ê°€ ë” ì»¤ì§„ ë•ë¶„ì— ê°ê° 2ì´ˆë°–ì— ê±¸ë¦¬ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì´ëŠ” ì§€ê¸ˆê¹Œì§€ í•™ìŠµí•œ ê°€ì¥ ë¹ ë¥¸ ëª¨ë¸ë³´ë‹¤ ëª‡ ë°°ë‚˜ ë¹ ë¥¸ ì†ë„ì…ë‹ˆë‹¤. ë˜í•œ ëª¨ë¸ì˜ ì„±ëŠ¥ë„ 50.7%ì—ì„œ 52.0%ë¡œ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤."""